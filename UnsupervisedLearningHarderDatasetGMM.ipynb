{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Note:</h2>\n",
    "\n",
    "This notebook contains optional part that is not related to Mozgalo competition.\n",
    "\n",
    "Notebook applies our trained model on a harder dataset with cca 30000 images and roughly 256 meaningful categories of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "import torchfile\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import binascii\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since dataset is much bigger it needed to be split in chunks\n",
    "dataset_chunk_0 = torchfile.load('256_set/featuresChunk0.t7')\n",
    "dataset_chunk_1 = torchfile.load('256_set/featuresChunk1.t7')\n",
    "dataset_chunk_2 = torchfile.load('256_set/featuresChunk2.t7')\n",
    "dataset_chunk_3 = torchfile.load('256_set/featuresChunk3.t7')\n",
    "dataset_chunk_4 = torchfile.load('256_set/featuresChunk4.t7')\n",
    "dataset_chunk_5 = torchfile.load('256_set/featuresChunk5.t7')\n",
    "dataset_chunk_6 = torchfile.load('256_set/featuresChunk6.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "dataset_list.append(dataset_chunk_0)\n",
    "dataset_list.append(dataset_chunk_1)\n",
    "dataset_list.append(dataset_chunk_2)\n",
    "dataset_list.append(dataset_chunk_3)\n",
    "dataset_list.append(dataset_chunk_4)\n",
    "dataset_list.append(dataset_chunk_5)\n",
    "dataset_list.append(dataset_chunk_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chunk gluing\n",
    "dataset = {}\n",
    "\n",
    "dataset['image_list'] = dataset_list[0]['image_list']\n",
    "dataset['features'] = dataset_list[0]['features']\n",
    "\n",
    "\n",
    "for i in range(1, len(dataset_list)):\n",
    "    dataset['image_list']+=dataset_list[i]['image_list']\n",
    "    dataset['features'] = np.concatenate((dataset['features'], dataset_list[i]['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30607\n"
     ]
    }
   ],
   "source": [
    "print (len(dataset['features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=1000, random_state=None,\n",
       "  svd_solver='full', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce to 1000 features\n",
    "pca = decomposition.PCA(n_components=1000, svd_solver='full')\n",
    "pca.fit(dataset['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_data = pca.transform(dataset['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=256, n_init=1, precisions_init=None,\n",
       "        random_state=None, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.mixture\n",
    "\n",
    "n_clusters = 256\n",
    "        \n",
    "clusterer = sklearn.mixture.GaussianMixture(n_clusters)\n",
    "clusterer.fit(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = clusterer.predict(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "#place results in this directory\n",
    "harder_dataset_dir_path = './256_dataset_GMM/'\n",
    "\n",
    "if not os.path.exists(harder_dataset_dir_path):\n",
    "        os.makedirs(harder_dataset_dir_path)\n",
    "        \n",
    "for i in range(n_clusters):\n",
    "    newpath = harder_dataset_dir_path + 'class_' + str(i)\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "for i in range(len(dataset['image_list'])):\n",
    "    if i % 10000 == 0:\n",
    "        print i\n",
    "    # directory with images is positioned in parent directory\n",
    "    path = '../' + dataset['image_list'][i]\n",
    "    \n",
    "    save_path = harder_dataset_dir_path + r'class_' + str(predictions[i])\n",
    "    tokens = path.decode(\"utf-8\").split('/')\n",
    "    copyfile(path.decode(\"utf-8\"), save_path + '/' + tokens[len(tokens) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "harder_dataset_dir_path = './256_dataset_GMM/'\n",
    "directories = []\n",
    "for file in os.listdir(harder_dataset_dir_path):\n",
    "    directories.append(harder_dataset_dir_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = []\n",
    "labels = []\n",
    "labels_true = []\n",
    "\n",
    "for i, class_dir in enumerate(directories):\n",
    "    class_id = class_dir.split('_')[-1]\n",
    "    images_in_dir = os.listdir(class_dir)\n",
    "    for image_in_dir in images_in_dir:\n",
    "        image_in_dir_name = image_in_dir.split('/')[-1]\n",
    "        for i, image in enumerate(dataset['image_list']):\n",
    "            file_name = dataset['image_list'][i].split('/')[-1]\n",
    "            if file_name == image_in_dir_name:\n",
    "                feature_vectors.append(dataset['features'][i])\n",
    "                labels.append(class_id)\n",
    "                labels_true.append(dataset['image_list'][i].split('_')[0].split('/')[-1])\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lab_tr = [int(elem) for elem in labels_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI for this dataset is 0.483129090198\n"
     ]
    }
   ],
   "source": [
    "print \"ARI for this dataset is\", metrics.adjusted_rand_score(lab_tr, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
