{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Intro:</h2>\n",
    "\n",
    "This is the implementation of the Mozgalo task.\n",
    "Dataset of features is loaded with torchfile library, which knows how to load already extracted features inside xxx.t7 file that contains the results of feature extraction done by [Torch](http://torch.ch/).\n",
    "\n",
    "[PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) is used to reduce the high dimensionality(Torch output is 2048 features per example image), and remove potential linear correlation between features.\n",
    "\n",
    "As an optimal clustering method it was decided for [GMM](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) to be used.\n",
    "\n",
    "\n",
    "All rights reserved © FerKani\n",
    "\n",
    "Goran Brlas\n",
    "\n",
    "Josip Silović\n",
    "\n",
    "Nikola Mrzljak\n",
    "\n",
    "Franjo Matković"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#needed imports\n",
    "\n",
    "import sklearn\n",
    "from sklearn import decomposition\n",
    "import torchfile\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import binascii\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import random\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6889, 2048)\n"
     ]
    }
   ],
   "source": [
    "#load whole dataset\n",
    "\n",
    "dataset = torchfile.load('DatasetFeatures.t7')\n",
    "print (dataset.features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=1000, random_state=None,\n",
       "  svd_solver='full', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 1000 # value decided by trial and error method, gives best results for clustering\n",
    "pca = decomposition.PCA(n_components=n_components, svd_solver='full')\n",
    "pca.fit(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_data = pca.transform(dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>WARNING</h1>\n",
    "\n",
    "Next cell can only be run after the files in the dataset have been formatted accordingly by the change.py script.\n",
    "\n",
    "The script needs to be centered next to the original dataset folder and NEEDS to be run <b>before</b> running the Torch feature extraction process.\n",
    "\n",
    "Run: python change.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Dataset/dataset/0001.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b694e376c2cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'./class_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/dreyus95/anaconda3/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Dataset/dataset/0001.jpg'"
     ]
    }
   ],
   "source": [
    "n_clusters = 10 # value chosen by trial and error method, 10 gives the optimal clusters representation\n",
    "\n",
    "transformed_data_unshuffled = pca.transform(dataset.features)\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    newpath = r'./class_' + str(i)\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "clusterer = GaussianMixture(n_components=n_clusters, random_state=12)\n",
    "clusterer.fit(transformed_data_unshuffled)\n",
    "\n",
    "for i in range(len(dataset.image_list)):\n",
    "    path = dataset.image_list[i]\n",
    "    \n",
    "    feature_vec = transformed_data_unshuffled[i]\n",
    "    class_val = clusterer.predict(feature_vec)\n",
    "    save_path = r'./class_' + str(class_val[0])\n",
    "    tokens = path.decode(\"utf-8\").split('/')\n",
    "    copyfile(path.decode(\"utf-8\"), save_path + '/' + tokens[len(tokens) - 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(18, 7)\n",
    "\n",
    "# The 1st subplot is the silhouette plot\n",
    "# The silhouette coefficient can range from -1, 1 but in this example all\n",
    "# lie within [-0.1, 1]\n",
    "ax1.set_xlim([-0.1, 1])\n",
    "# The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "# plots of individual clusters, to demarcate them clearly.\n",
    "ax1.set_ylim([0, len(transformed_data) + (n_clusters + 1) * 10])\n",
    "\n",
    "# Initialize the clusterer with n_clusters value and a random generator\n",
    "# seed of 10 for reproducibility.\n",
    "#clusterer = GaussianMixture(n_components=n_clusters)\n",
    "#clusterer.fit(transformed_data)\n",
    "cluster_labels = clusterer.predict(transformed_data)\n",
    "\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "# This gives a perspective into the density and separation of the formed\n",
    "# clusters\n",
    "silhouette_avg = silhouette_score(transformed_data, cluster_labels)\n",
    "print(\"For n_clusters =\", n_clusters,\n",
    "      \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(transformed_data, cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "    # Aggregate the silhouette scores for samples belonging to\n",
    "    # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = \\\n",
    "        sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                      0, ith_cluster_silhouette_values,\n",
    "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "    # Label the silhouette plots with their cluster numbers at the middle\n",
    "    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "    # Compute the new y_lower for next plot\n",
    "    y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "# The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "# 2nd Plot showing the actual clusters formed\n",
    "colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "ax2.scatter(transformed_data[:, 0], transformed_data[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "            c=colors)\n",
    "\n",
    "# Labeling the clusters\n",
    "centers = clusterer.means_\n",
    "# Draw white circles at cluster centers\n",
    "ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "            marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "ax2.set_title(\"The visualization of the clustered data.\")\n",
    "\n",
    "plt.suptitle((\"Silhouette analysis for GMM clustering on sample data \"\n",
    "              \"with n_clusters = %d\" % n_clusters),\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>END</h2>\n",
    "\n",
    "This marks the end of the Mozgalo task.\n",
    "\n",
    "The code bellow is a tryout code for word embeddings. As you can see, API that was used is not giving expected results for many clusters. While in some clusters it is able to deduce the correct target of the group(group 3 for example), in some clusters the result is a straightforward failure(group 7 for example).\n",
    "\n",
    "Used API: [NLTK](http://www.nltk.org/) ➡ [Wordnet](https://pythonprogramming.net/wordnet-nltk-tutorial/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Calculates n nearest examples for each centroid of the given dataset.\n",
    "    Returns them for each cluster in a list.\n",
    "\"\"\"\n",
    "def get_n_nearest_per_centroid(clusterer, n, torch_dataset):\n",
    "    data_per_class = []\n",
    "    data = pca.transform(torch_dataset.features)\n",
    "    image_names = np.asarray(torch_dataset.image_list)\n",
    "    np.matrix(image_names)\n",
    "    predictions = clusterer.predict(data)\n",
    "    data_with_probs = np.c_[image_names, predictions]\n",
    "    data_with_probs = np.c_[data_with_probs, clusterer.score_samples(data)[:]]\n",
    "    data_with_probs = sorted(data_with_probs, key=lambda e: e[-1]) \n",
    "    \n",
    "    clusters = set(predictions)\n",
    "    ct_clusters = len(clusters)\n",
    "\n",
    "    for cluster in range(ct_clusters):\n",
    "        indices = np.where(np.array(data_with_probs)[:,1] == str(cluster))\n",
    "        indices = indices[0]\n",
    "        data_per_class.append([])\n",
    "        for i in range(n):\n",
    "            data_per_class[cluster].append(data_with_probs[indices[i]])\n",
    "    return data_per_class\n",
    "\n",
    "#############################################################################\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "    Simple getter for n random examples for each cluster in given dataset.\n",
    "    Returns the list of n examples for each cluster.\n",
    "\"\"\"\n",
    "def get_n_random(clusterer, n, torch_dataset):\n",
    "    data_per_class = []\n",
    "    data = pca.transform(torch_dataset.features)\n",
    "    image_names = np.asarray(torch_dataset.image_list)\n",
    "    np.matrix(image_names)\n",
    "    predictions = clusterer.predict(data)\n",
    "    data_with_predictions = np.c_[image_names, predictions]\n",
    "    \n",
    "    clusters = set(predictions)  \n",
    "    ct_clusters = len(clusters)\n",
    "    \n",
    "    for cluster in range(ct_clusters):\n",
    "        indices = np.where(np.array(data_with_predictions)[:,1] == str(cluster))\n",
    "        indices = indices[random.randint(0, len(indices)-1)]\n",
    "        data_per_class.append([])\n",
    "        for i in range(n):\n",
    "            data_per_class[cluster].append(data_with_predictions[indices[i]])\n",
    "    return data_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nearest = get_n_nearest_per_centroid(clusterer, 15, dataset)\n",
    "random_representants = get_n_random(clusterer, 10, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Parser for input text.\n",
    "    Text contains the word embeddings that were given by wordnet API for examples.\n",
    "    Returns the list of embeddings.\n",
    "\"\"\"\n",
    "def process_torch_output(text):\n",
    "    word_list = []\n",
    "    lines = text.split('\\n')[1:]\n",
    "    \n",
    "    lines = lines[2:len(lines)-2]\n",
    "    \n",
    "    for line in lines:\n",
    "        words = line.split('\\t')[1:-1]\n",
    "        for i in range(len(words)):\n",
    "            words[i] = words[i].replace(' ', '')\n",
    "        all_words = []\n",
    "        for i in range(len(words)):\n",
    "            for word in words:\n",
    "                splitted = word.split(',')\n",
    "                for s in splitted:\n",
    "                    all_words.append(s)\n",
    "        for word in all_words:\n",
    "            word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cluster_words = {}\n",
    "\n",
    "for i, cluster_representants in enumerate(nearest):\n",
    "    cluster_words[i] = []\n",
    "    for representant in cluster_representants:\n",
    "        classifier = '../../resnet/fb.resnet.torch/pretrained/classify.lua'\n",
    "        resnet = '../../resnet/resnet-200.t7'\n",
    "        image = representant[0]\n",
    "        cluster_words[i] = cluster_words[i] + (process_torch_output(subprocess.check_output(['th', classifier, resnet, image])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "cluster_words = {}\n",
    "\n",
    "for i, cluster_representants in enumerate(random_representants):\n",
    "    cluster_words[i] = []\n",
    "    for representant in cluster_representants:\n",
    "        classifier = '../../resnet/fb.resnet.torch/pretrained/classify.lua'\n",
    "        resnet = '../../resnet/resnet-200.t7'\n",
    "        image = representant[0]\n",
    "        cluster_words[i] = cluster_words[i] + (process_torch_output(subprocess.check_output(['th', classifier, resnet, image])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nikola/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "0\n",
      "[u'aircraft', u'mechanism', u'object', u'lighter-than-air_craft']\n",
      "1\n",
      "[u'car', u'motor_vehicle', u'wheeled_vehicle', u'compartment']\n",
      "2\n",
      "[u'insect', u'hymenopterous_insect', u'change', u'structure']\n",
      "3\n",
      "[u'feline', u'big_cat', u'oil_well', u'attacker']\n",
      "4\n",
      "[u'course', u'support', u'shop', u'obtain']\n",
      "5\n",
      "[u'structure', u'region', u'environment', u'body_part']\n",
      "6\n",
      "[u'wheeled_vehicle', u'container', u'vehicle', u'ride']\n",
      "7\n",
      "[u'garment', u'clothing', u'food_fish', u'fish']\n",
      "8\n",
      "[u'mountain_sheep', u'wild_sheep', u'bovid', 'RockyMountainbighorn']\n",
      "9\n",
      "[u'act', u'give', u'wheeled_vehicle', u'part']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wn = wordnet.wordnet\n",
    "\n",
    "def get_up_to_second_level_hypernyms(word):\n",
    "    roots = set()\n",
    "    roots.add(word)\n",
    "    first_level_roots = set()\n",
    "    for i,j in enumerate(wn.synsets(word)):\n",
    "        for hyper in (j.hypernyms()):\n",
    "            first_level_roots.add(hyper.name().split('.')[0])\n",
    "    roots.update(first_level_roots)\n",
    "    for w in first_level_roots:\n",
    "        for i,j in enumerate(wn.synsets(w)):\n",
    "            for hyper in (j.hypernyms()):\n",
    "                roots.add(hyper.name().split('.')[0])\n",
    "    return roots\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_cluster_descriptor(cluster_words):\n",
    "    words = []\n",
    "    for word in cluster_words:\n",
    "        for w in get_up_to_second_level_hypernyms(word):\n",
    "            words.append(w)\n",
    "    return Counter(words)\n",
    "\n",
    "def get_n_highest(cntr, n):\n",
    "    best = []\n",
    "    for i in range(n):\n",
    "        highest = max(cntr, key=cntr.get)\n",
    "        best.append(highest)\n",
    "        del cntr[highest]\n",
    "    return best\n",
    "\n",
    "for i in range(10):\n",
    "    print i\n",
    "    cntr = get_cluster_descriptor(cluster_words[i])\n",
    "    print get_n_highest(cntr, 4)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
